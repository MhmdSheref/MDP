{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# üè≠ Enhanced RL Training for Perishable Inventory MDP\n",
        "\n",
        "**Professional Training Pipeline with State-of-the-Art Improvements**\n",
        "\n",
        "This notebook implements a production-ready RL training pipeline featuring:\n",
        "\n",
        "| Feature | Description |\n",
        "|---------|-------------|\n",
        "| **5M Training Steps** | Extended training for better convergence |\n",
        "| **Learning Rate Annealing** | Linear decay from 3e-4 ‚Üí 0 |\n",
        "| **Entropy Decay** | From 0.01 ‚Üí 0.001 for exploration/exploitation balance |\n",
        "| **Curriculum Learning** | Simple ‚Üí Moderate ‚Üí Complex ‚Üí Extreme |\n",
        "| **Cost-Aware Observations** | Enhanced state with supplier costs |\n",
        "| **Asymmetric Actions** | Favor cheap supplier ordering |\n",
        "| **TBS Benchmarking** | Continuous comparison with optimal baseline |\n",
        "| **100+ Environments** | Comprehensive evaluation suite |\n",
        "\n",
        "---\n",
        "\n",
        "**Objective**: Train an RL agent that outperforms the Tailored Base-Surge (TBS) policy on complex environments while matching performance on simple ones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toc"
      },
      "source": [
        "## üìã Table of Contents\n",
        "\n",
        "1. [Setup & Installation](#setup)\n",
        "2. [Environment Suite](#env-suite)\n",
        "3. [Training Configuration](#config)\n",
        "4. [Model Training](#training)\n",
        "5. [Evaluation & Benchmarking](#evaluation)\n",
        "6. [Results Analysis](#results)\n",
        "7. [Model Export](#export)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup-header"
      },
      "source": [
        "---\n",
        "## 1Ô∏è‚É£ Setup & Installation <a name=\"setup\"></a>\n",
        "\n",
        "Install dependencies and clone the repository."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install-deps"
      },
      "source": [
        "# Install dependencies\n",
        "!pip install stable-baselines3[extra] gymnasium numpy scipy matplotlib pandas tensorboard -q\n",
        "print(\"‚úÖ Dependencies installed\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clone-repo"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "# Check if running in Colab\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    # Clone repository\n",
        "    REPO_URL = \"https://github.com/MahmoudZah/Multi-Supplier-Perishable-Inventory.git\"\n",
        "    REPO_DIR = \"Multi-Supplier-Perishable-Inventory\"\n",
        "    \n",
        "    if not os.path.exists(REPO_DIR):\n",
        "        print(f\"üì• Cloning repository...\")\n",
        "        !git clone {REPO_URL} {REPO_DIR}\n",
        "    \n",
        "    os.chdir(REPO_DIR)\n",
        "    sys.path.insert(0, os.getcwd())\n",
        "    print(f\"üìÇ Working directory: {os.getcwd()}\")\n",
        "else:\n",
        "    print(\"üñ•Ô∏è Running locally\")\n",
        "\n",
        "print(\"‚úÖ Repository ready\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imports"
      },
      "source": [
        "# Core imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Dict, List, Optional, Any\n",
        "import time\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# RL imports\n",
        "import gymnasium as gym\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv\n",
        "from stable_baselines3.common.callbacks import CallbackList, EvalCallback, CheckpointCallback\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from gymnasium.wrappers import TimeLimit\n",
        "\n",
        "# Project imports\n",
        "from colab_training.gym_env import (\n",
        "    PerishableInventoryGymWrapper,\n",
        "    RewardConfig,\n",
        "    create_gym_env\n",
        ")\n",
        "from colab_training.environment_suite import (\n",
        "    EnvironmentSuite,\n",
        "    EnvironmentConfig,\n",
        "    create_environment_suite,\n",
        "    build_environment_from_config,\n",
        "    get_canonical_suite\n",
        ")\n",
        "from colab_training.callbacks import (\n",
        "    ScheduleCallback,\n",
        "    CurriculumCallback,\n",
        "    BenchmarkCallback,\n",
        "    create_lr_schedule,\n",
        "    create_entropy_schedule\n",
        ")\n",
        "from colab_training.benchmark import (\n",
        "    evaluate_policy,\n",
        "    compare_policies,\n",
        "    get_tbs_policy_for_env,\n",
        "    get_basestock_policy_for_env,\n",
        "    generate_performance_report,\n",
        "    visualize_comparison,\n",
        "    ComparisonReport\n",
        ")\n",
        "from perishable_inventory_mdp.policies import (\n",
        "    TailoredBaseSurgePolicy,\n",
        "    BaseStockPolicy,\n",
        "    DoNothingPolicy\n",
        ")\n",
        "\n",
        "print(\"‚úÖ All imports successful!\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpu-check"
      },
      "source": [
        "# Check GPU availability\n",
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    print(f\"üöÄ GPU available: {gpu_name}\")\n",
        "    print(f\"   CUDA version: {torch.version.cuda}\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"‚ö†Ô∏è No GPU available, using CPU\")\n",
        "    print(\"   Consider enabling GPU: Runtime ‚Üí Change runtime type ‚Üí GPU\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "env-suite-header"
      },
      "source": [
        "---\n",
        "## 2Ô∏è‚É£ Environment Suite <a name=\"env-suite\"></a>\n",
        "\n",
        "Load the canonical 105-environment benchmark suite with varying complexity levels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load-suite"
      },
      "source": [
        "# Load environment suite\n",
        "suite = get_canonical_suite()\n",
        "\n",
        "print(\"üìä Environment Suite Summary\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"Total environments: {len(suite)}\")\n",
        "print()\n",
        "\n",
        "summary = suite.get_summary()\n",
        "for complexity, count in sorted(summary.items()):\n",
        "    bar = \"‚ñà\" * (count // 2)\n",
        "    print(f\"  {complexity.capitalize():10s} ‚îÇ {count:3d} ‚îÇ {bar}\")\n",
        "\n",
        "print()\n",
        "print(\"Complexity Progression:\")\n",
        "print(\"  Simple   ‚Üí TBS-optimal scenarios (baseline)\")\n",
        "print(\"  Moderate ‚Üí Some seasonality/stochasticity\")\n",
        "print(\"  Complex  ‚Üí Composite demand, crisis dynamics\")\n",
        "print(\"  Extreme  ‚Üí Maximum challenge, RL should excel\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "preview-env"
      },
      "source": [
        "# Preview sample environment\n",
        "sample_config = suite.get_by_complexity(\"simple\")[0]\n",
        "print(\"üì¶ Sample Environment Configuration (Simple)\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"  Environment ID: {sample_config.env_id}\")\n",
        "print(f\"  Shelf life: {sample_config.shelf_life}\")\n",
        "print(f\"  Mean demand: {sample_config.mean_demand:.1f}\")\n",
        "print(f\"  Suppliers: {sample_config.num_suppliers}\")\n",
        "print(f\"  Lead times: {sample_config.lead_times}\")\n",
        "print(f\"  Unit costs: {sample_config.unit_costs}\")\n",
        "print(f\"  Demand type: {sample_config.demand_type}\")\n",
        "\n",
        "# Create and test environment\n",
        "test_env = create_gym_env(\n",
        "    shelf_life=sample_config.shelf_life,\n",
        "    mean_demand=sample_config.mean_demand,\n",
        "    fast_lead_time=sample_config.lead_times[0],\n",
        "    slow_lead_time=sample_config.lead_times[1],\n",
        "    fast_cost=sample_config.unit_costs[0],\n",
        "    slow_cost=sample_config.unit_costs[1]\n",
        ")\n",
        "\n",
        "print(f\"\\nüéÆ Gym Environment\")\n",
        "print(f\"  Observation space: {test_env.observation_space}\")\n",
        "print(f\"  Action space: {test_env.action_space}\")\n",
        "\n",
        "obs_info = test_env.get_observation_space_info()\n",
        "print(f\"\\nüìä Observation Components:\")\n",
        "for name, (start, end) in obs_info.items():\n",
        "    print(f\"    {name}: [{start}:{end}] ({end-start} dims)\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "config-header"
      },
      "source": [
        "---\n",
        "## 3Ô∏è‚É£ Training Configuration <a name=\"config\"></a>\n",
        "\n",
        "Configure the training hyperparameters for optimal performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "training-config"
      },
      "source": [
        "# üéõÔ∏è TRAINING CONFIGURATION\n",
        "# Adjust these based on available compute time\n",
        "\n",
        "TRAINING_CONFIG = {\n",
        "    # Core training\n",
        "    \"total_timesteps\": 5_000_000,       # 5M for full training (reduce for testing)\n",
        "    \"n_envs\": 8,                         # Parallel environments\n",
        "    \"episode_length\": 500,               # Steps per episode\n",
        "    \n",
        "    # Learning rate schedule\n",
        "    \"initial_learning_rate\": 3e-4,\n",
        "    \"final_learning_rate\": 0.0,\n",
        "    \n",
        "    # Entropy coefficient schedule\n",
        "    \"initial_entropy_coef\": 0.01,\n",
        "    \"final_entropy_coef\": 0.001,\n",
        "    \n",
        "    # Curriculum learning\n",
        "    \"curriculum_enabled\": True,\n",
        "    \"curriculum_thresholds\": {\n",
        "        \"simple\": -5.0,\n",
        "        \"moderate\": -8.0,\n",
        "        \"complex\": -12.0\n",
        "    },\n",
        "    \"min_episodes_per_level\": 50,\n",
        "    \n",
        "    # Evaluation & checkpointing\n",
        "    \"eval_freq\": 50_000,\n",
        "    \"checkpoint_freq\": 100_000,\n",
        "    \"benchmark_freq\": 100_000,\n",
        "    \"n_eval_episodes\": 10,\n",
        "    \n",
        "    # Model architecture\n",
        "    \"policy_kwargs\": {\n",
        "        \"net_arch\": [256, 256]           # Two hidden layers of 256 units\n",
        "    },\n",
        "    \n",
        "    # Random seed for reproducibility\n",
        "    \"seed\": 42\n",
        "}\n",
        "\n",
        "# Reward shaping configuration\n",
        "REWARD_CONFIG = RewardConfig(\n",
        "    alpha=0.5,       # Procurement cost weight\n",
        "    beta=0.3,        # Holding + spoilage weight\n",
        "    gamma=0.2,       # Shortage penalty weight\n",
        "    delta=0.1,       # Service bonus\n",
        "    target_fill_rate=0.95,\n",
        "    normalize=True,\n",
        "    normalization_scale=10.0\n",
        ")\n",
        "\n",
        "print(\"‚öôÔ∏è Training Configuration\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"  Total timesteps: {TRAINING_CONFIG['total_timesteps']:,}\")\n",
        "print(f\"  Parallel envs: {TRAINING_CONFIG['n_envs']}\")\n",
        "print(f\"  Episode length: {TRAINING_CONFIG['episode_length']}\")\n",
        "print()\n",
        "print(\"üìâ Learning Rate Schedule\")\n",
        "print(f\"  {TRAINING_CONFIG['initial_learning_rate']} ‚Üí {TRAINING_CONFIG['final_learning_rate']}\")\n",
        "print()\n",
        "print(\"üé≤ Entropy Schedule\")\n",
        "print(f\"  {TRAINING_CONFIG['initial_entropy_coef']} ‚Üí {TRAINING_CONFIG['final_entropy_coef']}\")\n",
        "print()\n",
        "print(\"üìö Curriculum Learning\")\n",
        "print(f\"  Enabled: {TRAINING_CONFIG['curriculum_enabled']}\")\n",
        "print(f\"  Thresholds: {TRAINING_CONFIG['curriculum_thresholds']}\")\n",
        "\n",
        "# Estimate training time\n",
        "steps_per_second = 1000  # Approximate\n",
        "estimated_hours = TRAINING_CONFIG['total_timesteps'] / steps_per_second / 3600\n",
        "print(f\"\\n‚è±Ô∏è Estimated training time: {estimated_hours:.1f} hours\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "training-header"
      },
      "source": [
        "---\n",
        "## 4Ô∏è‚É£ Model Training <a name=\"training\"></a>\n",
        "\n",
        "Train the PPO agent with curriculum learning and continuous benchmarking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "env-factory"
      },
      "source": [
        "def create_env_from_config(\n",
        "    env_config: EnvironmentConfig,\n",
        "    reward_config: Optional[RewardConfig] = None,\n",
        "    episode_length: int = 500\n",
        ") -> PerishableInventoryGymWrapper:\n",
        "    \"\"\"Create gym environment from EnvironmentConfig.\"\"\"\n",
        "    mdp = build_environment_from_config(env_config)\n",
        "    \n",
        "    env = PerishableInventoryGymWrapper(\n",
        "        mdp=mdp,\n",
        "        reward_config=reward_config or REWARD_CONFIG\n",
        "    )\n",
        "    \n",
        "    env = TimeLimit(env, max_episode_steps=episode_length)\n",
        "    env = Monitor(env)\n",
        "    \n",
        "    return env\n",
        "\n",
        "\n",
        "def make_curriculum_env_factory(n_envs: int, seed: int):\n",
        "    \"\"\"Create factory function for curriculum environments.\"\"\"\n",
        "    def env_factory(complexity: str) -> SubprocVecEnv:\n",
        "        configs = suite.get_by_complexity(complexity)\n",
        "        \n",
        "        if not configs:\n",
        "            raise ValueError(f\"No environments for complexity: {complexity}\")\n",
        "        \n",
        "        rng = np.random.RandomState(seed)\n",
        "        selected = rng.choice(configs, size=min(n_envs, len(configs)), replace=False).tolist()\n",
        "        \n",
        "        while len(selected) < n_envs:\n",
        "            selected.append(rng.choice(configs))\n",
        "        \n",
        "        def make_env(cfg):\n",
        "            def _init():\n",
        "                return create_env_from_config(\n",
        "                    cfg, \n",
        "                    REWARD_CONFIG, \n",
        "                    TRAINING_CONFIG['episode_length']\n",
        "                )\n",
        "            return _init\n",
        "        \n",
        "        env_fns = [make_env(cfg) for cfg in selected]\n",
        "        return SubprocVecEnv(env_fns) if n_envs > 1 else DummyVecEnv(env_fns)\n",
        "    \n",
        "    return env_factory\n",
        "\n",
        "\n",
        "def create_eval_env(complexity: str = \"simple\"):\n",
        "    \"\"\"Create evaluation environment.\"\"\"\n",
        "    configs = suite.get_by_complexity(complexity)\n",
        "    config = configs[0] if configs else suite.configs[0]\n",
        "    \n",
        "    def _init():\n",
        "        return create_env_from_config(\n",
        "            config, \n",
        "            REWARD_CONFIG, \n",
        "            TRAINING_CONFIG['episode_length']\n",
        "        )\n",
        "    \n",
        "    return DummyVecEnv([_init])\n",
        "\n",
        "print(\"‚úÖ Environment factories defined\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup-training"
      },
      "source": [
        "# Setup directories\n",
        "log_dir = Path(\"logs\")\n",
        "log_dir.mkdir(exist_ok=True)\n",
        "(log_dir / \"checkpoints\").mkdir(exist_ok=True)\n",
        "(log_dir / \"best_model\").mkdir(exist_ok=True)\n",
        "(log_dir / \"benchmark\").mkdir(exist_ok=True)\n",
        "\n",
        "# Create environment factory\n",
        "env_factory = make_curriculum_env_factory(\n",
        "    n_envs=TRAINING_CONFIG['n_envs'],\n",
        "    seed=TRAINING_CONFIG['seed']\n",
        ")\n",
        "\n",
        "# Create initial training environment (simple)\n",
        "print(\"üèóÔ∏è Creating training environment...\")\n",
        "train_env = env_factory(\"simple\")\n",
        "print(f\"   Starting with 'simple' complexity\")\n",
        "\n",
        "# Create evaluation environment\n",
        "eval_env = create_eval_env(\"simple\")\n",
        "print(f\"   Evaluation environment ready\")\n",
        "\n",
        "# Create learning rate schedule\n",
        "lr_schedule = create_lr_schedule(\n",
        "    TRAINING_CONFIG['initial_learning_rate'],\n",
        "    TRAINING_CONFIG['final_learning_rate']\n",
        ")\n",
        "\n",
        "# Create PPO model\n",
        "print(\"\\nüß† Creating PPO model...\")\n",
        "model = PPO(\n",
        "    \"MlpPolicy\",\n",
        "    train_env,\n",
        "    learning_rate=lr_schedule,\n",
        "    ent_coef=TRAINING_CONFIG['initial_entropy_coef'],\n",
        "    verbose=1,\n",
        "    tensorboard_log=str(log_dir / \"tensorboard\"),\n",
        "    seed=TRAINING_CONFIG['seed'],\n",
        "    device=\"auto\",\n",
        "    **{\"policy_kwargs\": TRAINING_CONFIG['policy_kwargs']}\n",
        ")\n",
        "\n",
        "print(f\"   Policy: {model.policy}\")\n",
        "print(f\"   Device: {model.device}\")\n",
        "print(\"\\n‚úÖ Model ready for training\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup-callbacks"
      },
      "source": [
        "# Setup callbacks\n",
        "callbacks = []\n",
        "\n",
        "# 1. Schedule callback (logs LR/entropy to tensorboard)\n",
        "schedule_callback = ScheduleCallback(\n",
        "    initial_lr=TRAINING_CONFIG['initial_learning_rate'],\n",
        "    final_lr=TRAINING_CONFIG['final_learning_rate'],\n",
        "    initial_ent_coef=TRAINING_CONFIG['initial_entropy_coef'],\n",
        "    final_ent_coef=TRAINING_CONFIG['final_entropy_coef'],\n",
        "    log_freq=5000,\n",
        "    verbose=1\n",
        ")\n",
        "callbacks.append(schedule_callback)\n",
        "print(\"üìâ Schedule callback: LR/entropy annealing\")\n",
        "\n",
        "# 2. Curriculum callback\n",
        "if TRAINING_CONFIG['curriculum_enabled']:\n",
        "    curriculum_callback = CurriculumCallback(\n",
        "        env_factory=env_factory,\n",
        "        thresholds=TRAINING_CONFIG['curriculum_thresholds'],\n",
        "        window_size=100,\n",
        "        min_episodes_per_level=TRAINING_CONFIG['min_episodes_per_level'],\n",
        "        verbose=1\n",
        "    )\n",
        "    callbacks.append(curriculum_callback)\n",
        "    print(\"üìö Curriculum callback: complexity progression\")\n",
        "\n",
        "# 3. Evaluation callback\n",
        "eval_callback = EvalCallback(\n",
        "    eval_env,\n",
        "    best_model_save_path=str(log_dir / \"best_model\"),\n",
        "    log_path=str(log_dir / \"eval\"),\n",
        "    eval_freq=TRAINING_CONFIG['eval_freq'] // TRAINING_CONFIG['n_envs'],\n",
        "    n_eval_episodes=TRAINING_CONFIG['n_eval_episodes'],\n",
        "    deterministic=True,\n",
        "    verbose=1\n",
        ")\n",
        "callbacks.append(eval_callback)\n",
        "print(\"üìä Evaluation callback: best model tracking\")\n",
        "\n",
        "# 4. Checkpoint callback\n",
        "checkpoint_callback = CheckpointCallback(\n",
        "    save_freq=TRAINING_CONFIG['checkpoint_freq'] // TRAINING_CONFIG['n_envs'],\n",
        "    save_path=str(log_dir / \"checkpoints\"),\n",
        "    name_prefix=\"ppo_perishable\"\n",
        ")\n",
        "callbacks.append(checkpoint_callback)\n",
        "print(\"üíæ Checkpoint callback: periodic saves\")\n",
        "\n",
        "# 5. Benchmark callback (TBS comparison)\n",
        "try:\n",
        "    tbs_policy = get_tbs_policy_for_env(eval_env)\n",
        "    benchmark_callback = BenchmarkCallback(\n",
        "        eval_env=eval_env,\n",
        "        benchmark_freq=TRAINING_CONFIG['benchmark_freq'] // TRAINING_CONFIG['n_envs'],\n",
        "        n_eval_episodes=TRAINING_CONFIG['n_eval_episodes'],\n",
        "        baseline_policies={\"TBS\": tbs_policy},\n",
        "        save_path=str(log_dir / \"benchmark\"),\n",
        "        verbose=1\n",
        "    )\n",
        "    callbacks.append(benchmark_callback)\n",
        "    print(\"üèÜ Benchmark callback: TBS comparison\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Could not create TBS baseline: {e}\")\n",
        "\n",
        "callback_list = CallbackList(callbacks)\n",
        "print(f\"\\n‚úÖ {len(callbacks)} callbacks configured\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "train"
      },
      "source": [
        "# üöÄ START TRAINING\n",
        "print(\"=\" * 60)\n",
        "print(\"üöÄ STARTING TRAINING\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Total timesteps: {TRAINING_CONFIG['total_timesteps']:,}\")\n",
        "print(f\"Parallel environments: {TRAINING_CONFIG['n_envs']}\")\n",
        "print(f\"Curriculum learning: {TRAINING_CONFIG['curriculum_enabled']}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "model.learn(\n",
        "    total_timesteps=TRAINING_CONFIG['total_timesteps'],\n",
        "    callback=callback_list,\n",
        "    progress_bar=True\n",
        ")\n",
        "\n",
        "training_time = time.time() - start_time\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"‚úÖ TRAINING COMPLETE\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Training time: {training_time/3600:.2f} hours\")\n",
        "print(f\"Steps per second: {TRAINING_CONFIG['total_timesteps']/training_time:.0f}\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "save-model"
      },
      "source": [
        "# Save final model\n",
        "final_model_path = log_dir / \"final_model\"\n",
        "model.save(str(final_model_path))\n",
        "print(f\"üíæ Final model saved to: {final_model_path}\")\n",
        "\n",
        "# Cleanup\n",
        "train_env.close()\n",
        "eval_env.close()\n",
        "print(\"‚úÖ Environments closed\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eval-header"
      },
      "source": [
        "---\n",
        "## 5Ô∏è‚É£ Evaluation & Benchmarking <a name=\"evaluation\"></a>\n",
        "\n",
        "Comprehensively evaluate the trained model against baselines across all environments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load-model"
      },
      "source": [
        "# Load best model\n",
        "best_model_path = log_dir / \"best_model\" / \"best_model.zip\"\n",
        "if best_model_path.exists():\n",
        "    model = PPO.load(str(best_model_path))\n",
        "    print(f\"‚úÖ Loaded best model from: {best_model_path}\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è Best model not found, using final model\")\n",
        "    model = PPO.load(str(final_model_path))"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "comprehensive-eval"
      },
      "source": [
        "# Comprehensive evaluation across all complexity levels\n",
        "print(\"üìä Comprehensive Evaluation\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "report = ComparisonReport()\n",
        "n_eval = 5  # Episodes per environment\n",
        "\n",
        "for complexity in [\"simple\", \"moderate\", \"complex\", \"extreme\"]:\n",
        "    configs = suite.get_by_complexity(complexity)\n",
        "    n_envs = min(5, len(configs))  # Evaluate on subset for speed\n",
        "    \n",
        "    print(f\"\\nüîç Evaluating {complexity.upper()} environments ({n_envs} samples)...\")\n",
        "    \n",
        "    for i, config in enumerate(configs[:n_envs]):\n",
        "        env = create_env_from_config(config, REWARD_CONFIG, 500)\n",
        "        \n",
        "        # Evaluate RL\n",
        "        rl_result = evaluate_policy(\n",
        "            policy=model,\n",
        "            env=env,\n",
        "            n_episodes=n_eval,\n",
        "            max_steps=500,\n",
        "            policy_name=\"RL\",\n",
        "            env_id=config.env_id,\n",
        "            complexity=complexity\n",
        "        )\n",
        "        report.add_result(rl_result)\n",
        "        \n",
        "        # Evaluate TBS\n",
        "        try:\n",
        "            tbs = get_tbs_policy_for_env(env)\n",
        "            tbs_result = evaluate_policy(\n",
        "                policy=tbs,\n",
        "                env=env,\n",
        "                n_episodes=n_eval,\n",
        "                max_steps=500,\n",
        "                policy_name=\"TBS\",\n",
        "                env_id=config.env_id,\n",
        "                complexity=complexity\n",
        "            )\n",
        "            report.add_result(tbs_result)\n",
        "        except:\n",
        "            pass\n",
        "        \n",
        "        env.close()\n",
        "        print(f\"   [{i+1}/{n_envs}] {config.env_id}: RL cost={rl_result.mean_cost:.1f}\")\n",
        "\n",
        "print(\"\\n‚úÖ Evaluation complete\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "results-header"
      },
      "source": [
        "---\n",
        "## 6Ô∏è‚É£ Results Analysis <a name=\"results\"></a>\n",
        "\n",
        "Analyze and visualize the performance comparison."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "results-summary"
      },
      "source": [
        "# Generate summary report\n",
        "print(generate_performance_report(report))"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "results-dataframe"
      },
      "source": [
        "# Show detailed results\n",
        "df = report.to_dataframe()\n",
        "print(\"\\nüìã Detailed Results\")\n",
        "display(df.head(20))"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "results-by-complexity"
      },
      "source": [
        "# Summary by complexity\n",
        "summary = report.get_summary_by_complexity()\n",
        "print(\"\\nüìä Summary by Complexity Level\")\n",
        "display(summary)"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "visualize"
      },
      "source": [
        "# Visualize comparison\n",
        "fig = visualize_comparison(report, save_path=str(log_dir / \"comparison.png\"))\n",
        "plt.show()"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rl-vs-tbs-ratio"
      },
      "source": [
        "# RL vs TBS cost ratio\n",
        "ratio = report.get_rl_vs_tbs_ratio()\n",
        "print(\"\\nüèÜ RL vs TBS Cost Ratio (lower is better for RL)\")\n",
        "print(\"=\" * 50)\n",
        "display(ratio)\n",
        "\n",
        "print(\"\\nInterpretation:\")\n",
        "print(\"  Ratio < 1.0 ‚Üí RL outperforms TBS\")\n",
        "print(\"  Ratio = 1.0 ‚Üí Equal performance\")\n",
        "print(\"  Ratio > 1.0 ‚Üí TBS outperforms RL\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "export-header"
      },
      "source": [
        "---\n",
        "## 7Ô∏è‚É£ Model Export <a name=\"export\"></a>\n",
        "\n",
        "Export the trained model and results for deployment or further analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "save-report"
      },
      "source": [
        "# Save evaluation report\n",
        "report_path = log_dir / \"evaluation_report.json\"\n",
        "report.save(str(report_path))\n",
        "print(f\"üìÑ Evaluation report saved: {report_path}\")\n",
        "\n",
        "# Save training config\n",
        "config_path = log_dir / \"training_config.json\"\n",
        "with open(config_path, 'w') as f:\n",
        "    json.dump(TRAINING_CONFIG, f, indent=2)\n",
        "print(f\"üìÑ Training config saved: {config_path}\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "download-colab"
      },
      "source": [
        "# Download files (Colab only)\n",
        "if IN_COLAB:\n",
        "    from google.colab import files\n",
        "    \n",
        "    # Zip logs directory\n",
        "    !zip -r logs.zip logs/\n",
        "    \n",
        "    print(\"\\nüì• Download trained model and results:\")\n",
        "    files.download('logs.zip')\n",
        "    print(\"\\n‚úÖ Download started\")\n",
        "else:\n",
        "    print(f\"\\nüìÅ All files saved in: {log_dir.absolute()}\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "conclusion"
      },
      "source": [
        "---\n",
        "## üéâ Training Complete!\n",
        "\n",
        "### Summary\n",
        "\n",
        "You have successfully trained a PPO agent on the Perishable Inventory MDP with:\n",
        "\n",
        "- ‚úÖ 5M training steps with learning rate annealing\n",
        "- ‚úÖ Curriculum learning through 4 complexity levels\n",
        "- ‚úÖ Cost-aware observations and asymmetric action space\n",
        "- ‚úÖ Comprehensive benchmarking against TBS baseline\n",
        "- ‚úÖ Evaluation across 100+ unique environments\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "1. **Analyze results**: Review the RL vs TBS comparison by complexity\n",
        "2. **Fine-tune**: Adjust hyperparameters if needed and retrain\n",
        "3. **Deploy**: Use the trained model for inventory optimization\n",
        "4. **Extend**: Add more complex scenarios or multi-item support\n",
        "\n",
        "### Files Generated\n",
        "\n",
        "| File | Description |\n",
        "|------|-------------|\n",
        "| `logs/final_model.zip` | Final trained model |\n",
        "| `logs/best_model/` | Best model during training |\n",
        "| `logs/checkpoints/` | Periodic checkpoints |\n",
        "| `logs/evaluation_report.json` | Full evaluation results |\n",
        "| `logs/comparison.png` | Visualization chart |"
      ]
    }
  ]
}