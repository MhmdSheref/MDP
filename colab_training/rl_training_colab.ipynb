{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# \ud83c\udfed Enhanced RL Training for Perishable Inventory MDP\n",
        "\n",
        "**Professional Training Pipeline with State-of-the-Art Improvements**\n",
        "\n",
        "This notebook implements a production-ready RL training pipeline featuring:\n",
        "\n",
        "| Feature | Description |\n",
        "|---------|-------------|\n",
        "| **5M Training Steps** | Extended training for better convergence |\n",
        "| **Learning Rate Annealing** | Linear decay from 3e-4 \u2192 0 |\n",
        "| **Entropy Decay** | From 0.01 \u2192 0.001 for exploration/exploitation balance |\n",
        "| **Curriculum Learning** | Simple \u2192 Moderate \u2192 Complex \u2192 Extreme |\n",
        "| **Cost-Aware Observations** | Enhanced state with supplier costs |\n",
        "| **Asymmetric Actions** | Favor cheap supplier ordering |\n",
        "| **TBS Benchmarking** | Continuous comparison with optimal baseline |\n",
        "| **100+ Environments** | Comprehensive evaluation suite |\n",
        "\n",
        "---\n",
        "\n",
        "**Objective**: Train an RL agent that outperforms the Tailored Base-Surge (TBS) policy on complex environments while matching performance on simple ones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toc"
      },
      "source": [
        "## \ud83d\udccb Table of Contents\n",
        "\n",
        "1. [Setup & Installation](#setup)\n",
        "2. [Environment Suite](#env-suite)\n",
        "3. [Training Configuration](#config)\n",
        "4. [Model Training](#training)\n",
        "5. [Evaluation & Benchmarking](#evaluation)\n",
        "6. [Results Analysis](#results)\n",
        "7. [Model Export](#export)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup-header"
      },
      "source": [
        "---\n",
        "## 1\ufe0f\u20e3 Setup & Installation <a name=\"setup\"></a>\n",
        "\n",
        "Install dependencies and clone the repository."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install-deps"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install stable-baselines3[extra] gymnasium numpy scipy matplotlib pandas tensorboard -q\n",
        "print(\"\u2705 Dependencies installed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clone-repo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "# Check if running in Colab\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    # Clone repository\n",
        "    REPO_URL = \"https://github.com/MahmoudZah/Multi-Supplier-Perishable-Inventory.git\"\n",
        "    REPO_DIR = \"Multi-Supplier-Perishable-Inventory\"\n",
        "    \n",
        "    if not os.path.exists(REPO_DIR):\n",
        "        print(f\"\ud83d\udce5 Cloning repository...\")\n",
        "        !git clone {REPO_URL} {REPO_DIR}\n",
        "    \n",
        "    os.chdir(REPO_DIR)\n",
        "    sys.path.insert(0, os.getcwd())\n",
        "    print(f\"\ud83d\udcc2 Working directory: {os.getcwd()}\")\n",
        "else:\n",
        "    print(\"\ud83d\udda5\ufe0f Running locally\")\n",
        "\n",
        "print(\"\u2705 Repository ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imports"
      },
      "outputs": [],
      "source": [
        "# Core imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Dict, List, Optional, Any\n",
        "import time\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# RL imports\n",
        "import gymnasium as gym\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv\n",
        "from stable_baselines3.common.callbacks import CallbackList, EvalCallback, CheckpointCallback\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from gymnasium.wrappers import TimeLimit\n",
        "\n",
        "# Project imports\n",
        "from colab_training.gym_env import (\n",
        "    PerishableInventoryGymWrapper,\n",
        "    RewardConfig,\n",
        "    create_gym_env\n",
        ")\n",
        "from colab_training.environment_suite import (\n",
        "    EnvironmentSuite,\n",
        "    EnvironmentConfig,\n",
        "    create_environment_suite,\n",
        "    build_environment_from_config,\n",
        "    get_canonical_suite\n",
        ")\n",
        "from colab_training.callbacks import (\n",
        "    ScheduleCallback,\n",
        "    CurriculumCallback,\n",
        "    BenchmarkCallback,\n",
        "    create_lr_schedule,\n",
        "    create_entropy_schedule\n",
        ")\n",
        "from colab_training.benchmark import (\n",
        "    evaluate_policy,\n",
        "    compare_policies,\n",
        "    get_tbs_policy_for_env,\n",
        "    get_basestock_policy_for_env,\n",
        "    generate_performance_report,\n",
        "    visualize_comparison,\n",
        "    ComparisonReport\n",
        ")\n",
        "from perishable_inventory_mdp.policies import (\n",
        "    TailoredBaseSurgePolicy,\n",
        "    BaseStockPolicy,\n",
        "    DoNothingPolicy\n",
        ")\n",
        "\n",
        "print(\"\u2705 All imports successful!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpu-check"
      },
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    print(f\"\ud83d\ude80 GPU available: {gpu_name}\")\n",
        "    print(f\"   CUDA version: {torch.version.cuda}\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"\u26a0\ufe0f No GPU available, using CPU\")\n",
        "    print(\"   Consider enabling GPU: Runtime \u2192 Change runtime type \u2192 GPU\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "env-suite-header"
      },
      "source": [
        "---\n",
        "## 2\ufe0f\u20e3 Environment Suite <a name=\"env-suite\"></a>\n",
        "\n",
        "Load the canonical 105-environment benchmark suite with varying complexity levels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load-suite"
      },
      "outputs": [],
      "source": [
        "# Load environment suite\n",
        "suite = get_canonical_suite()\n",
        "\n",
        "print(\"\ud83d\udcca Environment Suite Summary\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"Total environments: {len(suite)}\")\n",
        "print()\n",
        "\n",
        "summary = suite.get_summary()\n",
        "for complexity, count in sorted(summary.items()):\n",
        "    bar = \"\u2588\" * (count // 2)\n",
        "    print(f\"  {complexity.capitalize():10s} \u2502 {count:3d} \u2502 {bar}\")\n",
        "\n",
        "print()\n",
        "print(\"Complexity Progression:\")\n",
        "print(\"  Simple   \u2192 TBS-optimal scenarios (baseline)\")\n",
        "print(\"  Moderate \u2192 Some seasonality/stochasticity\")\n",
        "print(\"  Complex  \u2192 Composite demand, crisis dynamics\")\n",
        "print(\"  Extreme  \u2192 Maximum challenge, RL should excel\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "preview-env"
      },
      "outputs": [],
      "source": [
        "# Preview sample environment\n",
        "sample_config = suite.get_by_complexity(\"simple\")[0]\n",
        "print(\"\ud83d\udce6 Sample Environment Configuration (Simple)\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"  Environment ID: {sample_config.env_id}\")\n",
        "print(f\"  Shelf life: {sample_config.shelf_life}\")\n",
        "print(f\"  Mean demand: {sample_config.mean_demand:.1f}\")\n",
        "print(f\"  Suppliers: {sample_config.num_suppliers}\")\n",
        "print(f\"  Lead times: {sample_config.lead_times}\")\n",
        "print(f\"  Unit costs: {sample_config.unit_costs}\")\n",
        "print(f\"  Demand type: {sample_config.demand_type}\")\n",
        "\n",
        "# Create and test environment\n",
        "test_env = create_gym_env(\n",
        "    shelf_life=sample_config.shelf_life,\n",
        "    mean_demand=sample_config.mean_demand,\n",
        "    fast_lead_time=sample_config.lead_times[0],\n",
        "    slow_lead_time=sample_config.lead_times[1],\n",
        "    fast_cost=sample_config.unit_costs[0],\n",
        "    slow_cost=sample_config.unit_costs[1]\n",
        ")\n",
        "\n",
        "print(f\"\\n\ud83c\udfae Gym Environment\")\n",
        "print(f\"  Observation space: {test_env.observation_space}\")\n",
        "print(f\"  Action space: {test_env.action_space}\")\n",
        "\n",
        "obs_info = test_env.get_observation_space_info()\n",
        "print(f\"\\n\ud83d\udcca Observation Components:\")\n",
        "for name, (start, end) in obs_info.items():\n",
        "    print(f\"    {name}: [{start}:{end}] ({end-start} dims)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "config-header"
      },
      "source": [
        "---\n",
        "## 3\ufe0f\u20e3 Training Configuration <a name=\"config\"></a>\n",
        "\n",
        "Configure the training hyperparameters for optimal performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "training-config"
      },
      "outputs": [],
      "source": [
        "# \ud83c\udf9b\ufe0f TRAINING CONFIGURATION\n",
        "# Adjust these based on available compute time\n",
        "\n",
        "TRAINING_CONFIG = {\n",
        "    # Core training\n",
        "    \"total_timesteps\": 5_000_000,       # 5M for full training (reduce for testing)\n",
        "    \"n_envs\": 8,                         # Parallel environments\n",
        "    \"episode_length\": 500,               # Steps per episode\n",
        "    \n",
        "    # Learning rate schedule\n",
        "    \"initial_learning_rate\": 3e-4,\n",
        "    \"final_learning_rate\": 0.0,\n",
        "    \n",
        "    # Entropy coefficient schedule\n",
        "    \"initial_entropy_coef\": 0.01,\n",
        "    \"final_entropy_coef\": 0.001,\n",
        "    \n",
        "    # Curriculum learning\n",
        "    \"curriculum_enabled\": True,\n",
        "    \"curriculum_thresholds\": {\n",
        "        \"simple\": -5.0,\n",
        "        \"moderate\": -8.0,\n",
        "        \"complex\": -12.0\n",
        "    },\n",
        "    \"min_episodes_per_level\": 50,\n",
        "    \n",
        "    # Evaluation & checkpointing\n",
        "    \"eval_freq\": 50_000,\n",
        "    \"checkpoint_freq\": 100_000,\n",
        "    \"benchmark_freq\": 100_000,\n",
        "    \"n_eval_episodes\": 10,\n",
        "    \n",
        "    # Model architecture\n",
        "    \"policy_kwargs\": {\n",
        "        \"net_arch\": [256, 256]           # Two hidden layers of 256 units\n",
        "    },\n",
        "    \n",
        "    # Random seed for reproducibility\n",
        "    \"seed\": 42\n",
        "}\n",
        "\n",
        "# Reward shaping configuration\n",
        "REWARD_CONFIG = RewardConfig(\n",
        "    alpha=0.5,       # Procurement cost weight\n",
        "    beta=0.3,        # Holding + spoilage weight\n",
        "    gamma=0.2,       # Shortage penalty weight\n",
        "    delta=0.1,       # Service bonus\n",
        "    target_fill_rate=0.95,\n",
        "    normalize=True,\n",
        "    normalization_scale=10.0\n",
        ")\n",
        "\n",
        "print(\"\u2699\ufe0f Training Configuration\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"  Total timesteps: {TRAINING_CONFIG['total_timesteps']:,}\")\n",
        "print(f\"  Parallel envs: {TRAINING_CONFIG['n_envs']}\")\n",
        "print(f\"  Episode length: {TRAINING_CONFIG['episode_length']}\")\n",
        "print()\n",
        "print(\"\ud83d\udcc9 Learning Rate Schedule\")\n",
        "print(f\"  {TRAINING_CONFIG['initial_learning_rate']} \u2192 {TRAINING_CONFIG['final_learning_rate']}\")\n",
        "print()\n",
        "print(\"\ud83c\udfb2 Entropy Schedule\")\n",
        "print(f\"  {TRAINING_CONFIG['initial_entropy_coef']} \u2192 {TRAINING_CONFIG['final_entropy_coef']}\")\n",
        "print()\n",
        "print(\"\ud83d\udcda Curriculum Learning\")\n",
        "print(f\"  Enabled: {TRAINING_CONFIG['curriculum_enabled']}\")\n",
        "print(f\"  Thresholds: {TRAINING_CONFIG['curriculum_thresholds']}\")\n",
        "\n",
        "# Estimate training time\n",
        "steps_per_second = 1000  # Approximate\n",
        "estimated_hours = TRAINING_CONFIG['total_timesteps'] / steps_per_second / 3600\n",
        "print(f\"\\n\u23f1\ufe0f Estimated training time: {estimated_hours:.1f} hours\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "training-header"
      },
      "source": [
        "---\n",
        "## 4\ufe0f\u20e3 Model Training <a name=\"training\"></a>\n",
        "\n",
        "Train the PPO agent with curriculum learning and continuous benchmarking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "env-factory"
      },
      "outputs": [],
      "source": [
        "def create_env_from_config(\n",
        "    env_config: EnvironmentConfig,\n",
        "    reward_config: Optional[RewardConfig] = None,\n",
        "    episode_length: int = 500\n",
        ") -> PerishableInventoryGymWrapper:\n",
        "    \"\"\"Create gym environment from EnvironmentConfig.\"\"\"\n",
        "    mdp = build_environment_from_config(env_config)\n",
        "    \n",
        "    env = PerishableInventoryGymWrapper(\n",
        "        mdp=mdp,\n",
        "        reward_config=reward_config or REWARD_CONFIG\n",
        "    )\n",
        "    \n",
        "    env = TimeLimit(env, max_episode_steps=episode_length)\n",
        "    env = Monitor(env)\n",
        "    \n",
        "    # Store mdp reference on wrapper for TBS policy creation\n",
        "    env.mdp = mdp\n",
        "    \n",
        "    return env\n",
        "\n",
        "\n",
        "def make_curriculum_env_factory(n_envs: int, seed: int):\n",
        "    \"\"\"Create factory function for curriculum environments.\"\"\"\n",
        "    def env_factory(complexity: str) -> SubprocVecEnv:\n",
        "        configs = suite.get_by_complexity(complexity)\n",
        "        \n",
        "        if not configs:\n",
        "            raise ValueError(f\"No environments for complexity: {complexity}\")\n",
        "        \n",
        "        rng = np.random.RandomState(seed)\n",
        "        selected = rng.choice(configs, size=min(n_envs, len(configs)), replace=False).tolist()\n",
        "        \n",
        "        while len(selected) < n_envs:\n",
        "            selected.append(rng.choice(configs))\n",
        "        \n",
        "        def make_env(cfg):\n",
        "            def _init():\n",
        "                return create_env_from_config(\n",
        "                    cfg, \n",
        "                    REWARD_CONFIG, \n",
        "                    TRAINING_CONFIG['episode_length']\n",
        "                )\n",
        "            return _init\n",
        "        \n",
        "        env_fns = [make_env(cfg) for cfg in selected]\n",
        "        return SubprocVecEnv(env_fns) if n_envs > 1 else DummyVecEnv(env_fns)\n",
        "    \n",
        "    return env_factory\n",
        "\n",
        "\n",
        "def create_eval_env(complexity: str = \"simple\"):\n",
        "    \"\"\"Create evaluation environment.\"\"\"\n",
        "    configs = suite.get_by_complexity(complexity)\n",
        "    config = configs[0] if configs else suite.configs[0]\n",
        "    \n",
        "    def _init():\n",
        "        return create_env_from_config(\n",
        "            config, \n",
        "            REWARD_CONFIG, \n",
        "            TRAINING_CONFIG['episode_length']\n",
        "        )\n",
        "    \n",
        "    return DummyVecEnv([_init])\n",
        "\n",
        "print(\"\u2705 Environment factories defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup-training"
      },
      "outputs": [],
      "source": [
        "# Setup directories\n",
        "log_dir = Path(\"logs\")\n",
        "log_dir.mkdir(exist_ok=True)\n",
        "(log_dir / \"checkpoints\").mkdir(exist_ok=True)\n",
        "(log_dir / \"best_model\").mkdir(exist_ok=True)\n",
        "(log_dir / \"benchmark\").mkdir(exist_ok=True)\n",
        "\n",
        "# Create environment factory\n",
        "env_factory = make_curriculum_env_factory(\n",
        "    n_envs=TRAINING_CONFIG['n_envs'],\n",
        "    seed=TRAINING_CONFIG['seed']\n",
        ")\n",
        "\n",
        "# Create initial training environment (simple)\n",
        "print(\"\ud83c\udfd7\ufe0f Creating training environment...\")\n",
        "train_env = env_factory(\"simple\")\n",
        "print(f\"   Starting with 'simple' complexity\")\n",
        "\n",
        "# Create evaluation environment\n",
        "eval_env = create_eval_env(\"simple\")\n",
        "print(f\"   Evaluation environment ready\")\n",
        "\n",
        "# Create learning rate schedule\n",
        "lr_schedule = create_lr_schedule(\n",
        "    TRAINING_CONFIG['initial_learning_rate'],\n",
        "    TRAINING_CONFIG['final_learning_rate']\n",
        ")\n",
        "\n",
        "# Create PPO model\n",
        "print(\"\\n\ud83e\udde0 Creating PPO model...\")\n",
        "model = PPO(\n",
        "    \"MlpPolicy\",\n",
        "    train_env,\n",
        "    learning_rate=lr_schedule,\n",
        "    ent_coef=TRAINING_CONFIG['initial_entropy_coef'],\n",
        "    verbose=1,\n",
        "    tensorboard_log=str(log_dir / \"tensorboard\"),\n",
        "    seed=TRAINING_CONFIG['seed'],\n",
        "    device=\"auto\",\n",
        "    **{\"policy_kwargs\": TRAINING_CONFIG['policy_kwargs']}\n",
        ")\n",
        "\n",
        "print(f\"   Policy: {model.policy}\")\n",
        "print(f\"   Device: {model.device}\")\n",
        "print(\"\\n\u2705 Model ready for training\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup-callbacks"
      },
      "outputs": [],
      "source": [
        "# Setup callbacks\n",
        "callbacks = []\n",
        "\n",
        "# 1. Schedule callback (logs LR/entropy to tensorboard)\n",
        "schedule_callback = ScheduleCallback(\n",
        "    initial_lr=TRAINING_CONFIG['initial_learning_rate'],\n",
        "    final_lr=TRAINING_CONFIG['final_learning_rate'],\n",
        "    initial_ent_coef=TRAINING_CONFIG['initial_entropy_coef'],\n",
        "    final_ent_coef=TRAINING_CONFIG['final_entropy_coef'],\n",
        "    log_freq=5000,\n",
        "    verbose=1\n",
        ")\n",
        "callbacks.append(schedule_callback)\n",
        "print(\"\ud83d\udcc9 Schedule callback: LR/entropy annealing\")\n",
        "\n",
        "# 2. Curriculum callback\n",
        "if TRAINING_CONFIG['curriculum_enabled']:\n",
        "    curriculum_callback = CurriculumCallback(\n",
        "        env_factory=env_factory,\n",
        "        thresholds=TRAINING_CONFIG['curriculum_thresholds'],\n",
        "        window_size=100,\n",
        "        min_episodes_per_level=TRAINING_CONFIG['min_episodes_per_level'],\n",
        "        verbose=1\n",
        "    )\n",
        "    callbacks.append(curriculum_callback)\n",
        "    print(\"\ud83d\udcda Curriculum callback: complexity progression\")\n",
        "\n",
        "# 3. Evaluation callback\n",
        "eval_callback = EvalCallback(\n",
        "    eval_env,\n",
        "    best_model_save_path=str(log_dir / \"best_model\"),\n",
        "    log_path=str(log_dir / \"eval\"),\n",
        "    eval_freq=TRAINING_CONFIG['eval_freq'] // TRAINING_CONFIG['n_envs'],\n",
        "    n_eval_episodes=TRAINING_CONFIG['n_eval_episodes'],\n",
        "    deterministic=True,\n",
        "    verbose=1\n",
        ")\n",
        "callbacks.append(eval_callback)\n",
        "print(\"\ud83d\udcca Evaluation callback: best model tracking\")\n",
        "\n",
        "# 4. Checkpoint callback\n",
        "checkpoint_callback = CheckpointCallback(\n",
        "    save_freq=TRAINING_CONFIG['checkpoint_freq'] // TRAINING_CONFIG['n_envs'],\n",
        "    save_path=str(log_dir / \"checkpoints\"),\n",
        "    name_prefix=\"ppo_perishable\"\n",
        ")\n",
        "callbacks.append(checkpoint_callback)\n",
        "print(\"\ud83d\udcbe Checkpoint callback: periodic saves\")\n",
        "\n",
        "# 5. Benchmark callback (TBS comparison)\n",
        "try:\n",
        "    tbs_policy = get_tbs_policy_for_env(eval_env)\n",
        "    benchmark_callback = BenchmarkCallback(\n",
        "        eval_env=eval_env,\n",
        "        benchmark_freq=TRAINING_CONFIG['benchmark_freq'] // TRAINING_CONFIG['n_envs'],\n",
        "        n_eval_episodes=TRAINING_CONFIG['n_eval_episodes'],\n",
        "        baseline_policies={\"TBS\": tbs_policy},\n",
        "        save_path=str(log_dir / \"benchmark\"),\n",
        "        verbose=1\n",
        "    )\n",
        "    callbacks.append(benchmark_callback)\n",
        "    print(\"\ud83c\udfc6 Benchmark callback: TBS comparison\")\n",
        "except Exception as e:\n",
        "    print(f\"\u26a0\ufe0f Could not create TBS baseline: {e}\")\n",
        "\n",
        "callback_list = CallbackList(callbacks)\n",
        "print(f\"\\n\u2705 {len(callbacks)} callbacks configured\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "train"
      },
      "outputs": [],
      "source": [
        "# \ud83d\ude80 START TRAINING\n",
        "print(\"=\" * 60)\n",
        "print(\"\ud83d\ude80 STARTING TRAINING\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Total timesteps: {TRAINING_CONFIG['total_timesteps']:,}\")\n",
        "print(f\"Parallel environments: {TRAINING_CONFIG['n_envs']}\")\n",
        "print(f\"Curriculum learning: {TRAINING_CONFIG['curriculum_enabled']}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "model.learn(\n",
        "    total_timesteps=TRAINING_CONFIG['total_timesteps'],\n",
        "    callback=callback_list,\n",
        "    progress_bar=True\n",
        ")\n",
        "\n",
        "training_time = time.time() - start_time\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"\u2705 TRAINING COMPLETE\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Training time: {training_time/3600:.2f} hours\")\n",
        "print(f\"Steps per second: {TRAINING_CONFIG['total_timesteps']/training_time:.0f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "save-model"
      },
      "outputs": [],
      "source": [
        "# Save final model\n",
        "final_model_path = log_dir / \"final_model\"\n",
        "model.save(str(final_model_path))\n",
        "print(f\"\ud83d\udcbe Final model saved to: {final_model_path}\")\n",
        "\n",
        "# Cleanup\n",
        "train_env.close()\n",
        "eval_env.close()\n",
        "print(\"\u2705 Environments closed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "resume-header"
      },
      "source": [
        "---\n",
        "## \ud83d\udd04 Resume Training (If Interrupted)\n",
        "\n",
        "Use this cell if your training was interrupted (kernel crash, timeout, etc.).\n",
        "It will load the latest checkpoint and continue training from where it left off."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "resume-training"
      },
      "outputs": [],
      "source": [
        "# \ud83d\udd04 RESUME TRAINING FROM CHECKPOINT\n",
        "# Run this cell ONLY if training was interrupted and you want to continue\n",
        "\n",
        "import glob\n",
        "\n",
        "# Find latest checkpoint\n",
        "checkpoint_dir = Path(\"logs/checkpoints\")\n",
        "checkpoints = sorted(checkpoint_dir.glob(\"ppo_perishable_*_steps.zip\"))\n",
        "\n",
        "if not checkpoints:\n",
        "    print(\"\u274c No checkpoints found. Please run training from the beginning.\")\n",
        "else:\n",
        "    latest_checkpoint = checkpoints[-1]\n",
        "    print(f\"\ud83d\udcc2 Found {len(checkpoints)} checkpoints\")\n",
        "    print(f\"   Latest: {latest_checkpoint.name}\")\n",
        "    \n",
        "    # Extract step count from filename\n",
        "    checkpoint_steps = int(latest_checkpoint.stem.split(\"_\")[-2])\n",
        "    remaining_steps = TRAINING_CONFIG[\"total_timesteps\"] - checkpoint_steps\n",
        "    \n",
        "    print(f\"\\n\ud83d\udcca Progress:\")\n",
        "    print(f\"   Completed: {checkpoint_steps:,} steps\")\n",
        "    print(f\"   Remaining: {remaining_steps:,} steps\")\n",
        "    \n",
        "    if remaining_steps <= 0:\n",
        "        print(\"\\n\u2705 Training already complete! Proceed to evaluation.\")\n",
        "    else:\n",
        "        # Recreate environments (run env-factory and setup-training cells first if not done)\n",
        "        try:\n",
        "            train_env\n",
        "        except NameError:\n",
        "            print(\"\\n\u26a0\ufe0f Environments not initialized. Run these cells first:\")\n",
        "            print(\"   1. Training Configuration\")\n",
        "            print(\"   2. Environment factories\")\n",
        "            print(\"   3. Setup training\")\n",
        "            print(\"   Then come back here.\")\n",
        "        else:\n",
        "            # Load model with environment\n",
        "            print(f\"\\n\ud83e\udde0 Loading model from checkpoint...\")\n",
        "            model = PPO.load(str(latest_checkpoint), env=train_env)\n",
        "            print(f\"   Model loaded successfully\")\n",
        "            \n",
        "            # Resume training\n",
        "            print(f\"\\n\ud83d\ude80 Resuming training for {remaining_steps:,} more steps...\")\n",
        "            print(\"=\" * 60)\n",
        "            \n",
        "            start_time = time.time()\n",
        "            \n",
        "            model.learn(\n",
        "                total_timesteps=remaining_steps,\n",
        "                callback=callback_list,\n",
        "                reset_num_timesteps=False,  # Critical: continue step counter\n",
        "                progress_bar=True\n",
        "            )\n",
        "            \n",
        "            training_time = time.time() - start_time\n",
        "            print(\"\\n\" + \"=\" * 60)\n",
        "            print(\"\u2705 RESUMED TRAINING COMPLETE\")\n",
        "            print(\"=\" * 60)\n",
        "            print(f\"Additional training time: {training_time/3600:.2f} hours\")\n",
        "            \n",
        "            # Save final model\n",
        "            final_model_path = log_dir / \"final_model\"\n",
        "            model.save(str(final_model_path))\n",
        "            print(f\"\ud83d\udcbe Final model saved to: {final_model_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eval-header"
      },
      "source": [
        "---\n",
        "## 5\ufe0f\u20e3 Evaluation & Benchmarking <a name=\"evaluation\"></a>\n",
        "\n",
        "Comprehensively evaluate the trained model against baselines across all environments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load-model"
      },
      "outputs": [],
      "source": [
        "# Load best model\n",
        "best_model_path = log_dir / \"best_model\" / \"best_model.zip\"\n",
        "if best_model_path.exists():\n",
        "    model = PPO.load(str(best_model_path))\n",
        "    print(f\"\u2705 Loaded best model from: {best_model_path}\")\n",
        "else:\n",
        "    print(f\"\u26a0\ufe0f Best model not found, using final model\")\n",
        "    model = PPO.load(str(final_model_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "comprehensive-eval"
      },
      "outputs": [],
      "source": [
        "# Comprehensive evaluation across all complexity levels\n",
        "print(\"\ud83d\udcca Comprehensive Evaluation\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "report = ComparisonReport()\n",
        "n_eval = 5  # Episodes per environment\n",
        "\n",
        "for complexity in [\"simple\", \"moderate\", \"complex\", \"extreme\"]:\n",
        "    configs = suite.get_by_complexity(complexity)\n",
        "    n_envs = min(5, len(configs))  # Evaluate on subset for speed\n",
        "    \n",
        "    print(f\"\\n\ud83d\udd0d Evaluating {complexity.upper()} environments ({n_envs} samples)...\")\n",
        "    \n",
        "    for i, config in enumerate(configs[:n_envs]):\n",
        "        env = create_env_from_config(config, REWARD_CONFIG, 500)\n",
        "        \n",
        "        # Evaluate RL\n",
        "        rl_result = evaluate_policy(\n",
        "            policy=model,\n",
        "            env=env,\n",
        "            n_episodes=n_eval,\n",
        "            max_steps=500,\n",
        "            policy_name=\"RL\",\n",
        "            env_id=config.env_id,\n",
        "            complexity=complexity\n",
        "        )\n",
        "        report.add_result(rl_result)\n",
        "        \n",
        "        # Evaluate TBS\n",
        "        try:\n",
        "            tbs = get_tbs_policy_for_env(env)\n",
        "            tbs_result = evaluate_policy(\n",
        "                policy=tbs,\n",
        "                env=env,\n",
        "                n_episodes=n_eval,\n",
        "                max_steps=500,\n",
        "                policy_name=\"TBS\",\n",
        "                env_id=config.env_id,\n",
        "                complexity=complexity\n",
        "            )\n",
        "            report.add_result(tbs_result)\n",
        "        except:\n",
        "            pass\n",
        "        \n",
        "        env.close()\n",
        "        print(f\"   [{i+1}/{n_envs}] {config.env_id}: RL cost={rl_result.mean_cost:.1f}\")\n",
        "\n",
        "print(\"\\n\u2705 Evaluation complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "results-header"
      },
      "source": [
        "---\n",
        "## 6\ufe0f\u20e3 Results Analysis <a name=\"results\"></a>\n",
        "\n",
        "Analyze and visualize the performance comparison."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "results-summary"
      },
      "outputs": [],
      "source": [
        "# Generate summary report\n",
        "print(generate_performance_report(report))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "results-dataframe"
      },
      "outputs": [],
      "source": [
        "# Show detailed results\n",
        "df = report.to_dataframe()\n",
        "print(\"\\n\ud83d\udccb Detailed Results\")\n",
        "display(df.head(20))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "results-by-complexity"
      },
      "outputs": [],
      "source": [
        "# Summary by complexity\n",
        "summary = report.get_summary_by_complexity()\n",
        "print(\"\\n\ud83d\udcca Summary by Complexity Level\")\n",
        "display(summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "visualize"
      },
      "outputs": [],
      "source": [
        "# Visualize comparison\n",
        "fig = visualize_comparison(report, save_path=str(log_dir / \"comparison.png\"))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rl-vs-tbs-ratio"
      },
      "outputs": [],
      "source": [
        "# RL vs TBS cost ratio\n",
        "ratio = report.get_rl_vs_tbs_ratio()\n",
        "print(\"\\n\ud83c\udfc6 RL vs TBS Cost Ratio (lower is better for RL)\")\n",
        "print(\"=\" * 50)\n",
        "display(ratio)\n",
        "\n",
        "print(\"\\nInterpretation:\")\n",
        "print(\"  Ratio < 1.0 \u2192 RL outperforms TBS\")\n",
        "print(\"  Ratio = 1.0 \u2192 Equal performance\")\n",
        "print(\"  Ratio > 1.0 \u2192 TBS outperforms RL\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "export-header"
      },
      "source": [
        "---\n",
        "## 7\ufe0f\u20e3 Model Export <a name=\"export\"></a>\n",
        "\n",
        "Export the trained model and results for deployment or further analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "save-report"
      },
      "outputs": [],
      "source": [
        "# Save evaluation report\n",
        "report_path = log_dir / \"evaluation_report.json\"\n",
        "report.save(str(report_path))\n",
        "print(f\"\ud83d\udcc4 Evaluation report saved: {report_path}\")\n",
        "\n",
        "# Save training config\n",
        "config_path = log_dir / \"training_config.json\"\n",
        "with open(config_path, 'w') as f:\n",
        "    json.dump(TRAINING_CONFIG, f, indent=2)\n",
        "print(f\"\ud83d\udcc4 Training config saved: {config_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "download-colab"
      },
      "outputs": [],
      "source": [
        "# Download files (Colab only)\n",
        "if IN_COLAB:\n",
        "    from google.colab import files\n",
        "    \n",
        "    # Zip logs directory\n",
        "    !zip -r logs.zip logs/\n",
        "    \n",
        "    print(\"\\n\ud83d\udce5 Download trained model and results:\")\n",
        "    files.download('logs.zip')\n",
        "    print(\"\\n\u2705 Download started\")\n",
        "else:\n",
        "    print(f\"\\n\ud83d\udcc1 All files saved in: {log_dir.absolute()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "conclusion"
      },
      "source": [
        "---\n",
        "## \ud83c\udf89 Training Complete!\n",
        "\n",
        "### Summary\n",
        "\n",
        "You have successfully trained a PPO agent on the Perishable Inventory MDP with:\n",
        "\n",
        "- \u2705 5M training steps with learning rate annealing\n",
        "- \u2705 Curriculum learning through 4 complexity levels\n",
        "- \u2705 Cost-aware observations and asymmetric action space\n",
        "- \u2705 Comprehensive benchmarking against TBS baseline\n",
        "- \u2705 Evaluation across 100+ unique environments\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "1. **Analyze results**: Review the RL vs TBS comparison by complexity\n",
        "2. **Fine-tune**: Adjust hyperparameters if needed and retrain\n",
        "3. **Deploy**: Use the trained model for inventory optimization\n",
        "4. **Extend**: Add more complex scenarios or multi-item support\n",
        "\n",
        "### Files Generated\n",
        "\n",
        "| File | Description |\n",
        "|------|-------------|\n",
        "| `logs/final_model.zip` | Final trained model |\n",
        "| `logs/best_model/` | Best model during training |\n",
        "| `logs/checkpoints/` | Periodic checkpoints |\n",
        "| `logs/evaluation_report.json` | Full evaluation results |\n",
        "| `logs/comparison.png` | Visualization chart |"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "julia 1.11",
      "language": "julia",
      "name": "julia"
    },
    "language_info": {
      "name": "julia"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}